#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ë„¤ì´íŠ¸ ë‰´ìŠ¤ í¬ë¡¤ë§ ë””ë²„ê¹… ìŠ¤í¬ë¦½íŠ¸
"""

import requests
from bs4 import BeautifulSoup
import json

def debug_nate_news():
    """ë„¤ì´íŠ¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ êµ¬ì¡°ë¥¼ ë””ë²„ê¹…í•©ë‹ˆë‹¤."""
    
    # í…ŒìŠ¤íŠ¸í•  URLë“¤
    test_urls = [
        "https://news.nate.com/rank/interest?sc=eco",
        "https://news.nate.com/rank/interest?sc=spo", 
        "https://news.nate.com/rank/interest?sc=ent",
        "https://news.nate.com/rank/interest",  # ì „ì²´ ë­í‚¹
        "https://news.nate.com"  # ë©”ì¸ í˜ì´ì§€
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    for i, url in enumerate(test_urls, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ” í…ŒìŠ¤íŠ¸ {i}: {url}")
        print(f"{'='*60}")
        
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            print(f"âœ… ì‘ë‹µ ìƒíƒœ: {response.status_code}")
            print(f"ğŸ“„ ì‘ë‹µ ê¸¸ì´: {len(response.content)} bytes")
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # í˜ì´ì§€ ì œëª© í™•ì¸
            title = soup.find('title')
            if title:
                print(f"ğŸ“° í˜ì´ì§€ ì œëª©: {title.get_text()}")
            
            # ê°€ëŠ¥í•œ ë‰´ìŠ¤ ìš”ì†Œë“¤ ì°¾ê¸°
            print("\nğŸ” ë‰´ìŠ¤ ìš”ì†Œ ê²€ìƒ‰:")
            
            # ë‹¤ì–‘í•œ í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ë‰´ìŠ¤ ìš”ì†Œ ì°¾ê¸°
            possible_selectors = [
                'div.mduSubject',
                'div.newsSubject', 
                'div.subject',
                'div.news_item',
                'div.article',
                'a[href*="/view/"]',
                'a[href*="/article/"]',
                'div[class*="news"]',
                'div[class*="article"]',
                'div[class*="subject"]'
            ]
            
            for selector in possible_selectors:
                elements = soup.select(selector)
                if elements:
                    print(f"âœ… {selector}: {len(elements)}ê°œ ë°œê²¬")
                    if len(elements) <= 3:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                        for j, elem in enumerate(elements[:3]):
                            text = elem.get_text(strip=True)[:100]
                            print(f"   {j+1}. {text}...")
                else:
                    print(f"âŒ {selector}: ì—†ìŒ")
            
            # ë§í¬ë“¤ í™•ì¸
            print(f"\nğŸ”— ë§í¬ ë¶„ì„:")
            links = soup.find_all('a', href=True)
            news_links = [link for link in links if '/view/' in link['href'] or '/article/' in link['href']]
            print(f"ë‰´ìŠ¤ ë§í¬: {len(news_links)}ê°œ")
            
            if news_links:
                for j, link in enumerate(news_links[:3]):
                    href = link.get('href')
                    text = link.get_text(strip=True)[:50]
                    print(f"   {j+1}. {text} -> {href}")
            
            # HTML êµ¬ì¡° ì¼ë¶€ ì¶œë ¥
            print(f"\nğŸ“‹ HTML êµ¬ì¡° (ì²˜ìŒ 1000ì):")
            print(response.text[:1000])
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        print(f"\n{'='*60}")

if __name__ == "__main__":
    debug_nate_news()
